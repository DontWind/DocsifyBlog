# 1. 创建AudioContext

`AudioContext`的作用是关联音频输入，对音频进行解码、控制音频的播放暂停等基础操作。

```js
const AudioContext = window.AudioContext || window.webkitAudioContext
const AC = new AudioContext()
```

# 2. 创建AnalyserNode

`AnalyserNode`用于获取音频的频率数据（FrequencyData）和时域数据（TimeDomainData）。从而实现音频的可视化。

**只读取音频，不改变音频**

```js
const analyser = ctx.createAnalyser();
analyser.fftSize = 512
```

- `fftSize`  MDN:快速傅里叶变换的一个参数

  - 取值

    `fftSize` 的要求是**2的幂次方**。数值越大，得到的结果越精细。

    ```
    对于移动端网页来说，本身音频的比特率大多是 128Kbps ，没有必要用太大的频率数组去存储本身就不够精细的源数据。另外，手机屏幕的尺寸比桌面端小，因此最终展示图形也不需要每个频率都采到。只需要体现节奏即可，因此 512 是较为合理的值。
    ```

  - 作用
  
    决定 `frequencyData`的长度，具体为fftSize的**一半**

# 3.设置SourceNode

将音频节点关联到AudioContext上，作为整个音频分析过程的输入

在Web Audio中，有三种类型的音频源

- **MediaElementAudioSourceNode**：允许将<audio>节点直接作为输入，可做到流式播放

  ```js
  //获取audio节点
  const audio = document.getElementById('audio')
  //通过audio节点创建音频源
  const source = ctx.createMediaElementSource(audio)
  //将音频关联到分析器
  source.connect(analyser)
  //将分析器关联到输出设备
  analyser.connect(ctx.destination)
  ```

- **AudioBufferSourceNode**：通过xhr预先将音频文件加载下来，再用AudioContext进行解码

  ```
  有一种情况是，在安卓端，测试了在Chrome/69（不含）以下的版本，用 MediaElementAudioSourceNode 时，获取到的 frequencyData 是全为 0 的数组。
  
  因此，想要兼容这类机器，就需要换一种预加载的方式，即使用 AudioBufferSourceNode ，加载方式如下：
  // 创建一个xhr
  var xhr = new XMLHttpRequest();
  xhr.open('GET', '/path/to/audio.mp3', true);
  
  // 设置响应类型为 arraybuffer
  xhr.responseType = 'arraybuffer';
  
  xhr.onload = function() {
      var source = ctx.createBufferSource();
  
      // 对响应内容进行解码
      ctx.decodeAudioData(xhr.response, function(buffer) {
  
          // 将解码后得到的值赋给buffer
          source.buffer = buffer;
  
          // 完成。将source绑定到ctx。也可以连接AnalyserNode
          source.connect(ctx.destination);
      });
  };
  
  xhr.send();
  
  ```

  ![](https://pic2.zhimg.com/80/v2-0e2306fa38970c02deeaac9895b99c85_720w.jpg)

- **MediaStreamAudioSourceNode**：可以将用户的麦克风作为输入，即通过 `navigator.getUserMedia`获取用户的音或视频流后，生成音频源

# 4.播放音频

对于 `audio` 节点，即使用MediaElementAudioSourceNode的话，可直接调用 `audio.play()`进行播放

如果是AudioBufferSourceNode，它不存在play方法，而是：

```
const source = ctx.createBufferSource()
source.buffer = buffer
source.start(0)
```

# 5.获取frequencyData

获取音乐频率数据

关于频率，Web Audio提供了两个相关的API：

- analyser.getByteFrequencyData：返回的是0-255的Unit8Array。
- analyser.getFloatFrequencyData：返回的是0-22050的Float32Array

两者返回的都是TypedArray，唯一区别的是精度不同

```js
如何获取频率数组
const bufferLength = analyser.frequencyBinCount;
const dataArray = new Uint8Array(bufferLength);

analyser.getByteFrequencyData(dataArray);

是对已有的数组进行赋值，而不是创建后返回新的数组
好处是在代码中只会有一个dataArray引用，不用通过函数调用和参数传递的方式来重新取值
```

# 6.可视化

## 1.Canvas

Canvas本身是一个序列帧的播放，他在每一帧中，都要先清空Canvas，在重新绘制

![](https://pic1.zhimg.com/80/v2-a7c10f97678a30d1eb8b314be4250440_720w.jpg)

```js
function renderFrame() {
    requestAnimationFrame(renderFrame);

    // 更新频率数据
    analyser.getByteFrequencyData(dataArray);

    // bufferLength表示柱形图中矩形的个数
    for (var i = 0, x = 0; i < bufferLength; i++) {
        // 根据频率映射一个矩形高度
        barHeight = dataArray[i];

        // 根据每个矩形高度映射一个背景色
        var r = barHeight + 25 * (i / bufferLength);
        var g = 250 * (i / bufferLength);
        var b = 50;

        // 绘制一个矩形，并填充背景色
        ctx.fillStyle = "rgb(" + r + "," + g + "," + b + ")";
        ctx.fillRect(x, HEIGHT - barHeight, barWidth, barHeight);

        x += barWidth + 1;
    }
}

renderFrame();

Canvas 提供了丰富的绘制API，仅从 2D 的角度考虑，它也能实现很多酷炫的效果。类比 DOM 来说，如果只是<div>的组合就能做出丰富多彩的页面，那么 Canvas 一样可以。
```

对于可视化来说，核心逻辑在于：如何把频率数据映射成图形参数。

## 2.WebGL

Canvas是CPU计算，对于for循环计算10000次，而且每一帧都要重复计算，CPU是负载不了的。所以我们很少看到用Canvas 2D去实现粒子效果。取而代之的，是使用WebGL，借助GPU的计算能力。